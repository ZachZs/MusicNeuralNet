{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import spacy\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import ast\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Time Creation of Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the lyrics data set\n",
    "first_df = pd.read_csv(\"lyrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a song id number by renaming the index\n",
    "first_df.rename(columns={\"index\":\"song_id\"}, inplace=True)\n",
    "first_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_df.dropna(subset=['lyrics'], inplace=True)\n",
    "first_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataframe\n",
    "info = first_df[[\"song_id\", \"song\", \"year\", \"artist\", \"genre\"]]\n",
    "lyrics = first_df[[\"song_id\", \"lyrics\"]]\n",
    "\n",
    "print(info.columns)\n",
    "print(lyrics.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the spaCy object with the english corpus\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "experimenting with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a test case of one set of lyrics\n",
    "doc = nlp(lyrics.lyrics[0])\n",
    "\n",
    "# check the parts of speech etc\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)\n",
    "\n",
    "print(\"###################################################\")\n",
    "\n",
    "# check the tokenization\n",
    "for token in doc:\n",
    "    print(token.text)\n",
    "    \n",
    "print(\"###################################################\")\n",
    "\n",
    "# check the named entities in the document\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "\n",
    "print(\"###################################################\")\n",
    "\n",
    "# check if the words have vectors\n",
    "for token in doc:\n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating a tokenizer pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the lemmatizer function\n",
    "def lemmatizer(doc):\n",
    "    # remove the PRON (which are pronouns after lemming)\n",
    "    doc = [token.lemma_ for token in doc if token.lemma_ != '-PRON-']\n",
    "    doc = u' '.join(doc)\n",
    "    return nlp.make_doc(doc)\n",
    "\n",
    "def remove_punct(doc):\n",
    "    # remove punctuation -> Use token.text to return strings, needed for Gensim.\n",
    "    doc = [token.text for token in doc if token.is_punct != True]\n",
    "    return doc\n",
    "\n",
    "\n",
    "# add_pipe adds the function to the tokenizer\n",
    "nlp.add_pipe(lemmatizer,name='lemmatizer',after='ner')\n",
    "nlp.add_pipe(remove_punct, name=\"punct\", last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the column of lyrics\n",
    "doc = lyrics.lyrics\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see if any lyrics are missing\n",
    "doc.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# laptop time to run = \n",
    "# desktop time to run = \n",
    "\n",
    "doc_list = []\n",
    "\n",
    "# go through each song\n",
    "for doc in tqdm(doc):\n",
    "    \n",
    "    # tokenize the document\n",
    "    pr = nlp(doc)\n",
    "    \n",
    "    # add it to the list\n",
    "    doc_list.append(pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check a list of tokens\n",
    "doc_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn the list into a series, then turn it into a dataframe\n",
    "temp = pd.DataFrame(pd.Series(doc_list), columns=[\"tokens\"])\n",
    "\n",
    "# reset the indices of the original df and the token df\n",
    "temp.reset_index(drop=True, inplace=True)\n",
    "first_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# merge the tokens df onto the original df using the indicies\n",
    "df_with_tokens = pd.merge(first_df, temp, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the df\n",
    "df_with_tokens.to_csv(\"lyrics_with_tokens.txt\", sep=\"|\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in the Saved Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"lyrics_with_tokens.txt\", sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lyrics list and genre list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_list = df['tokens']\n",
    "lyrics_list = lyrics_list.to_list()\n",
    "\n",
    "lyr_list = []\n",
    "\n",
    "for lyr in lyrics_list:\n",
    "    temp = ast.literal_eval(lyr)\n",
    "    lyr_list.append(temp)\n",
    "    \n",
    "lyr_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_list = df['genre']\n",
    "genre_list = genre_list.to_list()\n",
    "\n",
    "gen_list = []\n",
    "\n",
    "for gen in genre_list:\n",
    "    gen_list.append(gen)\n",
    "    \n",
    "gen_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>create the tfidf (skip for now)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# create a dummy function to return the tokens as is\n",
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "# create the model for the tfidf, using the dummy function (since the tokens were made with spaCy)\n",
    "tfidf = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model to the tokens, and print the vocab\n",
    "tfidf_vector = tfidf.fit_transform(doc_list)\n",
    "tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape of the vector\n",
    "tfidf_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "\n",
    "# turn the sparse matrix into a dataframe\n",
    "pd.DataFrame.sparse.from_spmatrix(tfidf_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Word2Vec to train embeddings on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn the lyrics into a list of sentences (assuming each line is the equivilent of a sentence)\n",
    "\n",
    "# create a blank list\n",
    "result = []\n",
    "\n",
    "# loop through every lyric\n",
    "for i in lyr_list:\n",
    "    tmp = []\n",
    "    \n",
    "    # if the entry is a newline indicator, append nothing and start a new list tmp\n",
    "    for entry in i:\n",
    "        if entry != '\\n ':\n",
    "            tmp.append(entry)\n",
    "        else:\n",
    "            result.append(tmp)\n",
    "            tmp = []\n",
    "    result.append(tmp)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a counter for the index\n",
    "index_count = 0\n",
    "\n",
    "# loop through every sentence in the lyrics list\n",
    "for i in result:\n",
    "\n",
    "    # remove any chorus markers from the data (these are single item lists with the value chorus)\n",
    "    if len(i) != 1: \n",
    "        index_count = index_count + 1\n",
    "        continue\n",
    "    else:\n",
    "        if i[0] == 'chorus': \n",
    "            result.pop(index_count)\n",
    "        index_count = index_count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [[x.casefold() for x in sublst] for sublst in result]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "model = gensim.models.Word2Vec(result, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model['hello']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Doc2Vec to train embeddings on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Hierarchical Attention Network (HAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
